{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kY3jqh0s0lYa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 111940,
     "status": "ok",
     "timestamp": 1748618827072,
     "user": {
      "displayName": "Jules lablanche",
      "userId": "17156327266400456550"
     },
     "user_tz": -120
    },
    "id": "kY3jqh0s0lYa",
    "outputId": "bd2eca17-4de7-43d7-b260-11c5296728ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m105.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "# 0. Instalar librerÃ­as necesarias (solo la primera vez)\n",
    "!pip install transformers datasets evaluate rouge-score accelerate -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5LwZxKi90nAT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19988,
     "status": "ok",
     "timestamp": 1748618847058,
     "user": {
      "displayName": "Jules lablanche",
      "userId": "17156327266400456550"
     },
     "user_tz": -120
    },
    "id": "5LwZxKi90nAT",
    "outputId": "230c4b28-8199-4ee4-9aad-a79932dcea34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# 1. Montar Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d12293",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 813,
     "referenced_widgets": [
      "f3f07da3d3e54ea9892d51cf81f04dd9",
      "040545cdabc643ba86658f865ed3c981",
      "be2532b39daa4b5abc2ddb65c7f1359a",
      "b1a31fdf586c4985a2b3ede86492ea45",
      "ec31cd49092c4af19082770f83ecff45",
      "209cad5a611b40b29a3f51c2c3ce9892",
      "dfa0b8cfdb634644ba3ed360ffc6e7c8",
      "f447b335cf774bcebfbd643294689340",
      "cdb6b0fb2cc14dbba44c4821d4e851df",
      "83aff6a14c8a4da5bfe3f21ebc53bca2",
      "a479e9754c31498cb51e2cb5669de2ef",
      "ca0746d3f4a84ec18bdd89244f3cab1c",
      "200a6f6a9fc84cf880ae81b8dc341588",
      "bf44c7a9941944899bf52f0f41e24365",
      "841d9762b38b4c41b6186464ce1cf834",
      "466c716c35274abb910e28ad9986e7c7",
      "eade86fdb3db465cb58d9ec27b34e481",
      "2483c6108bd04e7195f04c3ad1425cce",
      "2ab4eb42b1fa4b169c88a51a92840971",
      "578e45d2cad944d190e828391f12e6ce",
      "9f40fe7f0c1547e69d2dfd9dfd077a9d",
      "3ce0e51994614713be99cfc42b7a19f4",
      "1d83ff55cb2c45bc96f07eed4404ac16",
      "34a438ff6c294c9abd76275b2933b057",
      "0c8a5d66be50494e8dcd1de1d805c80f",
      "6b9145896fa344e19399393b0c6b4466",
      "ec0f44e471f94898881362c84e6fb079",
      "0c9506a043aa4ad5840b445ac4c8fdeb",
      "7c8d3671e38b40babd49e298bc67919d",
      "b35595202a5f4904b10b0228666408e4",
      "ec1f83b9f0244e659755d50a89c3c1e3",
      "b234a35277e84271a37d7b27ab50a9aa",
      "08cc35e2a84c48a5b97680fbed871295",
      "ac2da4b837874751ac767d005d27a579",
      "e142fc35ca464a8a97876cbce5e6a858",
      "bcf5a5acc99d48ab8cd62c6253970abf",
      "2937ad5f81a2443e8ab13a58e2f8292c",
      "46bb49c387ef46809f13357f5225bff1",
      "91b93f56ed824af8b62cd64767a91f98",
      "e974610705494cbca4f83ea970d8987a",
      "a54654fcc23746ee9093cf2f0190b2e4",
      "02f1a91bb42249ddac769969325b5110",
      "7bca652887a74e11824cf5f4b6248bfc",
      "2113ccc3f0e5428d8e9002fac09c6406",
      "537ac40cddac4f8ab278e386aab28a7f",
      "e332eb2a0e8442888b3286a0b51b7f20",
      "2cb784257f9b4d4c8098ebb67958fcc3",
      "89e749eae24f427db8404d78946737fa",
      "5fc0f16aad744f2f964b0882b74d2826",
      "e9da3ee2c50049349f4f2c8bf69b46fa",
      "c30580eb25644428856cc2f778c08e9a",
      "0ff048c9010447c4895f9455c5f2f929",
      "08b72cf4f2a647af8cb3193f0e136dc8",
      "7cb7dbfe8ada4040a073e9a28df79d79",
      "36f1d0817dcf409eaa774c6bd846cf59",
      "720b446fcfd14025955b58d14b32b7e5",
      "b0d4da1e5f6b47019a3a744c225f334d",
      "51193b83f4a841f0b7ca43622712222c",
      "5de9feee89e344c0acf7d7c6b68afa1e",
      "3724605b6d2e42ad89b081bd82421185",
      "765c6b2695eb4770aaa0d31f61ac6084",
      "39c868b354034d3f9bca67b8a1c31c56",
      "4dfba0574f034b519d76575b4250cc72",
      "1dab9ff28f9a460f9780890f0bdde6b5",
      "5b9c3deadf7b473cbb60c84a2934b401",
      "09cba0c7a32e403d99de54723d8654bc",
      "a41b8335590d44e8b38166a0432030a7",
      "f7e44c58ce5d46d38ec292442cb5903c",
      "df96a7aca4b24c5093a7c62e3a9b28ad",
      "3b66748085b14f869c5ad303862dc800",
      "215f6bbc68064fe9b6dc4603e098ebb7",
      "4d9706d863294925a0e3dfae0b9bb0e7",
      "a422c9e3f2af4f2fa712f203f6b13555",
      "18470808def3426fbea6c52836eaaf07",
      "48e963f881a0412d8cdb5d60eeb71d41",
      "8a4bffa648154b0eabb5ff48e78b7c55",
      "3b85c55b978d4d97a687a94c2521e86d"
     ]
    },
    "executionInfo": {
     "elapsed": 351853,
     "status": "ok",
     "timestamp": 1748619198908,
     "user": {
      "displayName": "Jules lablanche",
      "userId": "17156327266400456550"
     },
     "user_tz": -120
    },
    "id": "a6d12293",
    "outputId": "7f3afc86-8790-4ca2-a69a-37b84b763e08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ No hay checkpoints vÃ¡lidos. Se carga el modelo base.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3f07da3d3e54ea9892d51cf81f04dd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca0746d3f4a84ec18bdd89244f3cab1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d83ff55cb2c45bc96f07eed4404ac16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac2da4b837874751ac767d005d27a579",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "537ac40cddac4f8ab278e386aab28a7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "720b446fcfd14025955b58d14b32b7e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a41b8335590d44e8b38166a0432030a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='105' max='105' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [105/105 04:07, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.981300</td>\n",
       "      <td>0.415570</td>\n",
       "      <td>0.821000</td>\n",
       "      <td>0.802700</td>\n",
       "      <td>0.816700</td>\n",
       "      <td>0.818600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.249400</td>\n",
       "      <td>0.172731</td>\n",
       "      <td>0.818600</td>\n",
       "      <td>0.800900</td>\n",
       "      <td>0.816500</td>\n",
       "      <td>0.816900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.127500</td>\n",
       "      <td>0.133958</td>\n",
       "      <td>0.824500</td>\n",
       "      <td>0.806300</td>\n",
       "      <td>0.821400</td>\n",
       "      <td>0.823500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:3464: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:33]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Resultados ROUGE en test: {'eval_loss': 0.15179197490215302, 'eval_rouge1': 0.7381, 'eval_rouge2': 0.7122, 'eval_rougeL': 0.7396, 'eval_rougeLsum': 0.7385, 'eval_runtime': 38.1133, 'eval_samples_per_second': 0.525, 'eval_steps_per_second': 0.262, 'epoch': 3.0}\n",
      "\n",
      "--- Ejemplo de prueba ---\n",
      "Texto original:\n",
      " Through unwavering determination, Galileo Galilei became a symbol of resistance against oppression. Their actions led to major reforms and left an enduring legacy in the fight for freedom and human rights.\n",
      "Resumen generado:\n",
      " Through unwavering determination, Galileo Galilei became a symbol of resistance against oppression. A symbolic scene captures the essence of this historical turning point. This moment could be captured in a vivid historical scene or historical scene. This could be the perfect scene for a historical reenactment.\n",
      "\n",
      "âœ… Resumen guardado en: /content/drive/MyDrive/LLMPractica/textfinetunned/resumen_generado.txt\n"
     ]
    }
   ],
   "source": [
    "# 2. Importaciones necesarias\n",
    "import os, glob\n",
    "import torch\n",
    "import evaluate\n",
    "from transformers import (\n",
    "    BartTokenizer, BartForConditionalGeneration,\n",
    "    Seq2SeqTrainer, Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# Cargar datasets preprocesados\n",
    "base_path = \"/content/drive/MyDrive/LLMPractica/preprocessedatasets\"\n",
    "train_dataset = load_from_disk(os.path.join(base_path, \"train_dataset\"))\n",
    "val_dataset = load_from_disk(os.path.join(base_path, \"val_dataset\"))\n",
    "test_dataset = load_from_disk(os.path.join(base_path, \"test_dataset\"))\n",
    "\n",
    "# Checkpoints\n",
    "checkpoint_dir = \"/content/drive/MyDrive/LLMPractica/bart-json-output\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "checkpoint_paths = sorted(glob.glob(os.path.join(checkpoint_dir, \"checkpoint-*\")), reverse=True)\n",
    "last_checkpoint = next((p for p in checkpoint_paths if os.path.isfile(os.path.join(p, \"pytorch_model.bin\"))), None)\n",
    "\n",
    "# Cargar modelo y tokenizer\n",
    "if last_checkpoint:\n",
    "    print(f\"ğŸ”„ Cargando modelo desde checkpoint vÃ¡lido: {last_checkpoint}\")\n",
    "    model = BartForConditionalGeneration.from_pretrained(last_checkpoint)\n",
    "    tokenizer = BartTokenizer.from_pretrained(last_checkpoint)\n",
    "else:\n",
    "    print(\"ğŸš€ No hay checkpoints vÃ¡lidos. Se carga el modelo base.\")\n",
    "    model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "    tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "\n",
    "# MÃ©trica ROUGE\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "def compute_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    return {k: round(v, 4) for k, v in rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True).items()}\n",
    "\n",
    "# ConfiguraciÃ³n del entrenamiento\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=checkpoint_dir,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"/content/drive/MyDrive/LLMPractica/logs\",\n",
    "    logging_steps=10,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=128,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model),\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Entrenar\n",
    "trainer.train(resume_from_checkpoint=last_checkpoint)\n",
    "\n",
    "# Evaluar\n",
    "results = trainer.evaluate(test_dataset)\n",
    "print(\"\\nğŸ“Š Resultados ROUGE en test:\", results)\n",
    "\n",
    "# Prueba de resumen\n",
    "def summarize(text):\n",
    "    input_text = \"Summary: \" + text\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    output_ids = model.generate(**inputs, max_length=128, num_beams=4, no_repeat_ngram_size=3, repetition_penalty=1.2)\n",
    "    return tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# Prueba con un texto del test y guardar el resumen\n",
    "print(\"\\n--- Ejemplo de prueba ---\")\n",
    "sample_text = test_dataset[10][\"text\"]\n",
    "resumen_generado = summarize(sample_text)\n",
    "\n",
    "print(\"Texto original:\\n\", sample_text)\n",
    "print(\"Resumen generado:\\n\", resumen_generado)\n",
    "\n",
    "# Guardar el resumen generado\n",
    "output_dir = \"/content/drive/MyDrive/LLMPractica/textfinetunned\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_path = os.path.join(output_dir, \"resumen_generado.txt\")\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(resumen_generado)\n",
    "\n",
    "print(f\"\\nâœ… Resumen guardado en: {output_path}\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ef8796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo aparte de QLoRA y cuantificaciÃ³n (para futuros proyectos)\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch\n",
    "\n",
    "# Cargar modelo base con cuantificaciÃ³n (4 bits)\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"  # Cambia por tu modelo\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_4bit=True,  # CuantificaciÃ³n 4 bits\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# ConfiguraciÃ³n LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Aplicar QLoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Ejemplo de inferencia\n",
    "inputs = tokenizer(\"Ejemplo de texto\", return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
